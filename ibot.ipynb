{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "\n",
    "file_q = h5py.File(\"datasets/jetnet/q.hdf5\", 'r')\n",
    "file_g = h5py.File(\"datasets/jetnet/g.hdf5\", 'r')\n",
    "file_w = h5py.File(\"datasets/jetnet/w.hdf5\", 'r')\n",
    "file_z = h5py.File(\"datasets/jetnet/z.hdf5\", 'r')\n",
    "file_t = h5py.File(\"datasets/jetnet/t.hdf5\", 'r')\n",
    "x_q = file_q['particle_features'][:]\n",
    "x_g = file_g['particle_features'][:]\n",
    "x_w = file_w['particle_features'][:]\n",
    "x_z = file_z['particle_features'][:]\n",
    "x_t = file_t['particle_features'][:]\n",
    "\n",
    "y_q = np.tile([1,0,0,0,0], (x_q.shape[0], 1))\n",
    "y_g = np.tile([0,1,0,0,0], (x_g.shape[0], 1))\n",
    "y_w = np.tile([0,0,1,0,0], (x_w.shape[0], 1))\n",
    "y_z = np.tile([0,0,0,1,0], (x_z.shape[0], 1))\n",
    "y_t = np.tile([0,0,0,0,1], (x_t.shape[0], 1))\n",
    "\n",
    "x_all = np.concatenate([x_q, x_g, x_w, x_z, x_t], axis=0)\n",
    "y_all = np.concatenate([y_q, y_g, y_w, y_z, y_t], axis=0)\n",
    "\n",
    "indices = np.arange(x_all.shape[0])\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_all = x_all[indices]\n",
    "y_all = y_all[indices]\n",
    "\n",
    "n_train = 100000\n",
    "n_val = 1000\n",
    "#n_test = x_all.shape[0] - n_train - n_val\n",
    "n_test = 100000\n",
    "\n",
    "x_train = x_all[:n_train]\n",
    "y_train = y_all[:n_train]\n",
    "\n",
    "x_val = x_all[n_train:n_train + n_val]\n",
    "y_val = y_all[n_train:n_train + n_val]\n",
    "\n",
    "x_test = x_all[n_train + n_val:]\n",
    "y_test = y_all[n_train + n_val:]\n",
    "\n",
    "x_test = x_test[:n_test]\n",
    "y_test = y_test[:n_test]\n",
    "\n",
    "del x_all, y_all\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "class_names = ['q', 'g', 'W', 'Z', 't']\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_idx = np.where(y_train.argmax(axis=1)==i)[0][0]\n",
    "    jet = x_train[class_idx]\n",
    "    eta = jet[:,0]\n",
    "    phi = jet[:,1]\n",
    "    pt = jet[:,2]\n",
    "    mask = jet[:,3]\n",
    "    \n",
    "    eta = eta[mask==1]\n",
    "    phi = phi[mask==1]\n",
    "    pt = pt[mask==1]\n",
    "\n",
    "    size = pt*10000\n",
    "\n",
    "    axs[i].scatter(eta, phi, color='C0', s=size, alpha=0.5)\n",
    "    axs[i].set_xlabel(\"Eta\")\n",
    "    axs[i].set_ylabel(\"Phi\")\n",
    "    axs[i].set_title(class_name)\n",
    "    axs[i].set_xlim(-0.4, 0.4)\n",
    "    axs[i].set_ylim(-0.4, 0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- augmentations\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "def rotate_eta_phi(jet):\n",
    "    theta = RNG.uniform(-np.pi, np.pi)\n",
    "    cos_t, sin_t = np.cos(theta), np.sin(theta)\n",
    "\n",
    "    eta, phi, pt, valid = jet.T\n",
    "    eta_r = cos_t*eta - sin_t*phi\n",
    "    phi_r = sin_t*eta + cos_t*phi\n",
    "    jet_r = np.vstack([eta_r, phi_r, pt, valid]).T\n",
    "    return jet_r\n",
    "\n",
    "def gaussian_smear_eta_phi(jet):\n",
    "    eta, phi, pt, valid = jet.T\n",
    "    sigma = 3e-4 / np.clip(pt, 1e-6, None) # 1e-4 = (QCD_scale 100 MeV) / (1 TeV); pt is particle pt / jet pt\n",
    "    eta_s = eta + RNG.normal(0., sigma)\n",
    "    phi_s = phi + RNG.normal(0., sigma)\n",
    "    phi_s = np.where(phi_s > np.pi, phi_s - 2*np.pi,\n",
    "            np.where(phi_s <= -np.pi, phi_s + 2*np.pi, phi_s))\n",
    "    jet_s = np.vstack([eta_s, phi_s, pt, valid]).T\n",
    "    return jet_s\n",
    "\n",
    "def split_particles(jet):\n",
    "    eta, phi, pt, valid = jet.T\n",
    "    valid_idx = np.where(valid==1)[0]\n",
    "    n_valid = valid_idx.size\n",
    "    n_empty = 30 - n_valid\n",
    "    if n_empty==0:\n",
    "        return jet.copy().astype(np.float32)\n",
    "\n",
    "    n_split  = min(n_empty, n_valid)\n",
    "    to_split = RNG.choice(valid_idx, n_split, replace=False)\n",
    "\n",
    "    extra = []\n",
    "    for idx in to_split:\n",
    "        a = RNG.uniform(0.3, 0.7)\n",
    "        pt1 = a*pt[idx]\n",
    "        pt2 = (1-a)*pt[idx]\n",
    "        pt[idx] = pt1\n",
    "        extra.append([eta[idx], phi[idx], pt2, 1.])\n",
    "\n",
    "    jet_new = np.concatenate([jet, np.array(extra, dtype=np.float32)], axis=0)\n",
    "\n",
    "    if jet_new.shape[0]<30:\n",
    "        pad = np.zeros((30 - jet_new.shape[0], 4), dtype=np.float32)\n",
    "        jet_new = np.concatenate([jet_new, pad], axis=0)\n",
    "    return jet_new[:30].astype(np.float32)\n",
    "\n",
    "def augment_jet(jet):\n",
    "    jet = rotate_eta_phi(jet)\n",
    "    jet = gaussian_smear_eta_phi(jet)\n",
    "    jet = split_particles(jet)\n",
    "    return jet.astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
    "class_names = ['q', 'g', 'W', 'Z', 't']\n",
    "mask_ratio = 0.4\n",
    "\n",
    "for col, cname in enumerate(class_names):\n",
    "    idx = np.where(y_train.argmax(axis=1)==col)[0][0]\n",
    "    jet_orig = x_train[idx]\n",
    "\n",
    "    # original\n",
    "    eta, phi, pt, valid = jet_orig.T\n",
    "    axes[0, col].scatter(eta[valid==1], phi[valid==1], color='C0', s=pt[valid==1]*1e4, alpha=0.5)\n",
    "    axes[0, col].set_title(f\"{cname} [original]\")\n",
    "\n",
    "    # augmented\n",
    "    jet_aug = augment_jet(jet_orig)\n",
    "    eta, phi, pt, valid = jet_aug.T\n",
    "    axes[1, col].scatter(eta[valid==1], phi[valid==1], color='C1', s=pt[valid==1]*1e4, alpha=0.5)\n",
    "    axes[1, col].set_title(f\"{cname} [augmented]\")\n",
    "\n",
    "    # augmented + masks\n",
    "    eta, phi, pt, valid = jet_aug.T\n",
    "    valid_idx = np.where(valid==1)[0]\n",
    "    n_valid = valid_idx.size\n",
    "    n_mask = max(1, int(np.ceil(mask_ratio * n_valid)))\n",
    "    mask_idx = RNG.choice(valid_idx, n_mask, replace=False)\n",
    "\n",
    "    mask_bool = np.zeros_like(pt, dtype=bool)\n",
    "    mask_bool[mask_idx] = True\n",
    "\n",
    "    pt_masked = pt.copy()\n",
    "    pt_masked[mask_bool] = 0\n",
    "\n",
    "    unmasked = (valid==1) & (~mask_bool)\n",
    "    masked = (valid==1) & mask_bool\n",
    "\n",
    "    axes[2, col].scatter(eta[unmasked], phi[unmasked], color='C1', s=pt_masked[unmasked]*1e4, alpha=0.5)\n",
    "    axes[2, col].scatter(eta[masked], phi[masked], marker='x', color='blue', s=50, linewidths=1, alpha=0.5)\n",
    "    axes[2, col].set_title(f\"{cname} [augmented+masked]\")\n",
    "\n",
    "    for row in range(3):\n",
    "        axes[row, col].set_xlim(-0.4, 0.4)\n",
    "        axes[row, col].set_ylim(-0.4, 0.4)\n",
    "        axes[row, col].set_xlabel(\"eta_rel\")\n",
    "        axes[row, col].set_ylabel(\"phi_rel\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6593fd4",
   "metadata": {},
   "source": [
    "## build ibot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddClsToken(Layer):\n",
    "    # add [CLS] token: (batch, n_tokens, d_model) -> (batch, n_tokens+1, d_model)\n",
    "    def __init__(self, d_model, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.cls = self.add_weight(name=\"cls_token\", shape=(1, 1, d_model), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        # tile along the batch dimension\n",
    "        cls_token = tf.tile(self.cls, [tf.shape(x)[0], 1, 1])\n",
    "        return tf.concat([cls_token, x], axis=1)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, d_model, n_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mha = MultiHeadAttention(n_heads, d_model//n_heads)\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn = tf.keras.Sequential([Dense(4*d_model, activation='relu'), Dense(d_model)])\n",
    "        self.drop1 = Dropout(0.2)\n",
    "        self.drop2 = Dropout(0.2)\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_out = self.mha(x, x)\n",
    "        attn_out = self.ln1(x + self.drop1(attn_out))\n",
    "\n",
    "        ffn_out = self.ffn(attn_out)\n",
    "        ffn_out = self.ln2(attn_out + self.drop2(ffn_out))\n",
    "        return ffn_out\n",
    "\n",
    "def build_backbone(d_model, n_heads, n_layers, name=\"backbone\"):\n",
    "    x_in = keras.Input((30, 4), name='particles_in') \n",
    "\n",
    "    # linear projection for token embedding\n",
    "    x = Dense(d_model, name=\"token_embedding\")(x_in)\n",
    "\n",
    "    # prepend [CLS] token\n",
    "    x = AddClsToken(d_model, name=\"prepend_cls\")(x)\n",
    "\n",
    "    # transformer blocks\n",
    "    for i in range(n_layers):\n",
    "        x = TransformerBlock(d_model, n_heads, name=f\"block_{i}\")(x)\n",
    "\n",
    "    # split CLS and patch (particle) outputs\n",
    "    cls_out = x[:, 0]\n",
    "    particle_out = x[:, 1:]\n",
    "    return keras.Model(x_in, [cls_out, particle_out], name=name)\n",
    "\n",
    "def build_proj_head(d_in, d_proj, name):\n",
    "    head = keras.Sequential([\n",
    "        Dense(d_proj, activation='relu'),\n",
    "        Dense(d_proj),\n",
    "        Lambda(lambda t: tf.math.l2_normalize(t, -1))\n",
    "    ], name=name)\n",
    "    head.build((None, None, d_in))\n",
    "    return head\n",
    "\n",
    "d_model=80\n",
    "n_heads=3\n",
    "n_layers=3\n",
    "d_proj=64\n",
    "\n",
    "student = build_backbone(d_model=d_model, n_heads=n_heads, n_layers=n_layers)\n",
    "teacher = build_backbone(d_model=d_model, n_heads=n_heads, n_layers=n_layers)\n",
    "\n",
    "proj_head_s = build_proj_head(d_in=d_model, d_proj=d_proj, name=\"proj_head_student\")\n",
    "proj_head_t = build_proj_head(d_in=d_model, d_proj=d_proj, name=\"proj_head_teacher\")\n",
    "\n",
    "teacher.set_weights(student.get_weights())\n",
    "proj_head_t.set_weights(proj_head_s.get_weights())\n",
    "teacher.trainable = False\n",
    "proj_head_t.trainable = False\n",
    "\n",
    "print(student.summary())\n",
    "print(proj_head_s.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrappers for tf.data\n",
    "def tf_augment_batch(jets):\n",
    "    def aug_(j):\n",
    "        out = tf.numpy_function(augment_jet, [j], tf.float32)\n",
    "        out.set_shape((30, 4))\n",
    "        return out\n",
    "    return tf.map_fn(aug_, jets, fn_output_signature=tf.TensorSpec((30,4), tf.float32))\n",
    "\n",
    "def tf_make_masks(jets, mask_ratio_ibot):\n",
    "    def mask_(jet):\n",
    "        eta, phi, pt, valid = tf.unstack(jet, axis=1)\n",
    "        valid_idx = tf.where(tf.equal(valid, 1.0))[:, 0]\n",
    "        n_valid = tf.size(valid_idx)\n",
    "        n_mask = tf.maximum(1, tf.cast(tf.math.ceil(mask_ratio_ibot * tf.cast(n_valid, tf.float32)), tf.int32))\n",
    "        mask_idx = tf.random.shuffle(valid_idx)[:n_mask]\n",
    "        mask_bool = tf.scatter_nd(tf.expand_dims(mask_idx, 1), tf.ones_like(mask_idx, tf.bool), [30])\n",
    "        return mask_bool\n",
    "\n",
    "    masks = tf.map_fn(mask_, jets, fn_output_signature=tf.TensorSpec((30,), tf.bool))\n",
    "    return masks\n",
    "\n",
    "# centering for teacher logits\n",
    "def update_center(tok_cls, tok_patch, center_cls, center_patch, center_beta):\n",
    "    center_cls.assign(center_beta * center_cls + (1 - center_beta) * tf.reduce_mean(tok_cls, axis=0))\n",
    "    center_patch.assign(center_beta * center_patch + (1 - center_beta) * tf.reduce_mean(tok_patch, axis=[0, 1]))\n",
    "\n",
    "# loss def\n",
    "def softmax_temp(x, t):\n",
    "    return tf.nn.softmax(x/t, axis=-1)\n",
    "\n",
    "def ce_teacher_student(t_logits, s_logits, temp_t=0.04, temp_s=0.1):\n",
    "    p_t = tf.stop_gradient(softmax_temp(t_logits, temp_t))\n",
    "    log_p_s = tf.nn.log_softmax(s_logits / temp_s, axis=-1)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(p_t * log_p_s, axis=-1))\n",
    "    return cross_entropy\n",
    "\n",
    "def ibot_loss(cls_s, cls_t, patch_s, patch_t, mask_bool, center_cls, center_patch):\n",
    "    # CLS\n",
    "    loss_cls = ce_teacher_student(cls_t - center_cls, cls_s)\n",
    "\n",
    "    # masked tokens\n",
    "    float_mask = tf.cast(mask_bool, tf.float32)[..., None]\n",
    "    patch_s = patch_s * float_mask\n",
    "    patch_t = (patch_t - center_patch) * float_mask\n",
    "\n",
    "    # flatten everything\n",
    "    patch_s_flat = tf.reshape(patch_s, [-1, d_proj])\n",
    "    patch_t_flat = tf.reshape(patch_t, [-1, d_proj])\n",
    "    bool_flat  = tf.reshape(mask_bool, [-1])\n",
    "\n",
    "    # keep only masked positions\n",
    "    patch_s_sel = tf.boolean_mask(patch_s_flat, bool_flat)\n",
    "    patch_t_sel = tf.boolean_mask(patch_t_flat, bool_flat)\n",
    "\n",
    "    loss_tok = ce_teacher_student(patch_t_sel, patch_s_sel)\n",
    "    return loss_cls + loss_tok, loss_cls, loss_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26613453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ibot(x_train, epochs, batch_size, optimizer, ema_tau,\n",
    "               student, teacher, proj_head_s, proj_head_t,\n",
    "               mask_ratio_ibot, masker, center_cls, center_patch,\n",
    "               center_beta, temp_t, temp_s):\n",
    "\n",
    "    token_emb_layer = student.get_layer(\"token_embedding\")\n",
    "    prepend_cls_layer = student.get_layer(\"prepend_cls\")\n",
    "    blocks = [student.get_layer(f\"block_{i}\") for i in range(n_layers)]\n",
    "\n",
    "    def ema_update(ema_tau=ema_tau):\n",
    "        for s, t in zip(student.weights, teacher.weights):\n",
    "            t.assign(ema_tau * t + (1 - ema_tau) * s)\n",
    "        for s, t in zip(proj_head_s.weights, proj_head_t.weights):\n",
    "            t.assign(ema_tau * t + (1 - ema_tau) * s)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(ds_jets, optimizer, student, teacher, proj_head_s, proj_head_t, masker):\n",
    "        # two augmented views\n",
    "        jet_u = tf_augment_batch(ds_jets)\n",
    "        jet_v = tf_augment_batch(ds_jets)\n",
    "\n",
    "        # tokens to mask\n",
    "        mask_u = tf_make_masks(jet_u, mask_ratio_ibot)\n",
    "        mask_v = tf_make_masks(jet_v, mask_ratio_ibot)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # student: replace masked tokens with learnable mask vector\n",
    "            emb_u = token_emb_layer(jet_u)\n",
    "            emb_v = token_emb_layer(jet_v)\n",
    "            emb_u_masked = masker(emb_u, mask_u)\n",
    "            emb_v_masked = masker(emb_v, mask_v)\n",
    "\n",
    "            xu = prepend_cls_layer(emb_u_masked)\n",
    "            xv = prepend_cls_layer(emb_v_masked)\n",
    "            for blk in blocks:\n",
    "                xu = blk(xu)\n",
    "                xv = blk(xv)\n",
    "\n",
    "            cls_u_s, patch_u_s = xu[:, 0], xu[:, 1:]\n",
    "            cls_v_s, patch_v_s = xv[:, 0], xv[:, 1:]\n",
    "\n",
    "            # projections\n",
    "            cls_u_s = proj_head_s(tf.expand_dims(cls_u_s, 1))[:, 0, :]\n",
    "            cls_v_s = proj_head_s(tf.expand_dims(cls_v_s, 1))[:, 0, :]\n",
    "            patch_u_s = proj_head_s(patch_u_s)\n",
    "            patch_v_s = proj_head_s(patch_v_s)\n",
    "\n",
    "            # teacher\n",
    "            cls_u_t, patch_u_t = teacher(jet_u, training=False)\n",
    "            cls_v_t, patch_v_t = teacher(jet_v, training=False)\n",
    "            cls_u_t = proj_head_t(tf.expand_dims(cls_u_t, 1))[:, 0, :]\n",
    "            cls_v_t = proj_head_t(tf.expand_dims(cls_v_t, 1))[:, 0, :]\n",
    "            patch_u_t = proj_head_t(patch_u_t)\n",
    "            patch_v_t = proj_head_t(patch_v_t)\n",
    "\n",
    "            # loss\n",
    "            loss_uv, loss_cls_uv, loss_patch_uv = ibot_loss(\n",
    "                cls_s=cls_u_s, cls_t=cls_v_t,\n",
    "                patch_s=patch_u_s, patch_t=patch_v_t,\n",
    "                mask_bool=mask_u, center_cls=center_cls, center_patch=center_patch\n",
    "            )\n",
    "            loss_vu, loss_cls_vu, loss_patch_vu = ibot_loss(\n",
    "                cls_s=cls_v_s, cls_t=cls_u_t,\n",
    "                patch_s=patch_v_s, patch_t=patch_u_t,\n",
    "                mask_bool=mask_v, center_cls=center_cls, center_patch=center_patch\n",
    "            )\n",
    "            loss = 0.5 * (loss_uv + loss_vu)\n",
    "            loss_cls = 0.5 * (loss_cls_uv + loss_cls_vu)\n",
    "            loss_patch = 0.5 * (loss_patch_uv + loss_patch_vu)\n",
    "\n",
    "            # optional can remove?\n",
    "            #loss += 1e-7 * tf.reduce_sum(masker.mask_token ** 2)\n",
    "\n",
    "        vars_ = student.trainable_weights + proj_head_s.trainable_weights + masker.trainable_weights\n",
    "        grads = tape.gradient(loss, vars_)\n",
    "        grads_vars = [(g, v) for g, v in zip(grads, vars_) if g is not None]\n",
    "        if grads_vars:\n",
    "            optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "        # entropy\n",
    "        z_t = tf.concat([cls_u_t, cls_v_t], axis=0)\n",
    "        prob_t = tf.nn.softmax((z_t - center_cls)[...] / temp_t, axis=-1)\n",
    "        entropy_teacher = -tf.reduce_mean(tf.reduce_sum(prob_t * tf.math.log(prob_t + 1e-9), axis=-1))\n",
    "        prob_s = tf.nn.softmax((cls_u_s + cls_v_s) / 2 / temp_s, axis=-1)\n",
    "        entropy_student = -tf.reduce_mean(tf.reduce_sum(prob_s * tf.math.log(prob_s + 1e-9), axis=-1))\n",
    "\n",
    "        # centers\n",
    "        update_center(\n",
    "            tok_cls=tf.stop_gradient(tf.concat([cls_u_t, cls_v_t], 0)),\n",
    "            tok_patch=tf.stop_gradient(tf.concat([patch_u_t, patch_v_t], 0)),\n",
    "            center_cls=center_cls, center_patch=center_patch, center_beta=center_beta\n",
    "        )\n",
    "        return loss, loss_cls, loss_patch, entropy_teacher, entropy_student\n",
    "\n",
    "    history = {\n",
    "        \"loss_total\": [], \"loss_cls\": [], \"loss_patch\": [],\n",
    "        \"entropy_teacher\": [], \"entropy_student\": [],\n",
    "        \"center_cls\": [], \"center_patch\": [],\n",
    "        \"mask_token\": []\n",
    "    }\n",
    "\n",
    "    ds = (tf.data.Dataset.from_tensor_slices(x_train.astype(\"float32\"))\n",
    "          .shuffle(x_train.shape[0], seed=42)\n",
    "          .batch(batch_size)\n",
    "          .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ema_tau_waved = 1 - (1 - ema_tau) * (1 + math.cos(math.pi * epoch / epochs)) / 2\n",
    "        L_total, L_cls, L_patch, E_teacher, E_student = [], [], [], [], []\n",
    "        for ds_jets in ds:\n",
    "            l_total, l_cls, l_patch, e_teacher, e_student = train_step(\n",
    "                ds_jets=ds_jets, optimizer=optimizer,\n",
    "                student=student, teacher=teacher,\n",
    "                proj_head_s=proj_head_s, proj_head_t=proj_head_t,\n",
    "                masker=masker\n",
    "            )\n",
    "            ema_update(ema_tau_waved)\n",
    "            L_total.append(l_total.numpy())\n",
    "            L_cls.append(l_cls.numpy())\n",
    "            L_patch.append(l_patch.numpy())\n",
    "            E_teacher.append(e_teacher.numpy())\n",
    "            E_student.append(e_student.numpy())\n",
    "\n",
    "        history[\"loss_total\"].append(np.mean(L_total))\n",
    "        history[\"loss_cls\"].append(np.mean(L_cls))\n",
    "        history[\"loss_patch\"].append(np.mean(L_patch))\n",
    "        history[\"entropy_teacher\"].append(np.mean(E_teacher))\n",
    "        history[\"entropy_student\"].append(np.mean(E_student))\n",
    "        history[\"center_cls\"].append(np.linalg.norm(center_cls.numpy()))\n",
    "        history[\"center_patch\"].append(np.linalg.norm(center_patch.numpy()))\n",
    "        history[\"mask_token\"].append(np.linalg.norm(masker.mask_token.numpy()))\n",
    "\n",
    "        print(f\"epoch {epoch+1}/{epochs} | \"\n",
    "              f\"loss(total)={history['loss_total'][-1]:.3f}; \"\n",
    "              f\"loss(cls)={history['loss_cls'][-1]:.3f}; \"\n",
    "              f\"loss(patch)={history['loss_patch'][-1]:.3f}; \"\n",
    "              f\"entropy(teacher)={history['entropy_teacher'][-1]:.3f}; \"\n",
    "              f\"entropy(student)={history['entropy_student'][-1]:.3f}; \"\n",
    "              f\"norm(center_cls)={history['center_cls'][-1]:.3f}; \"\n",
    "              f\"norm(center_patch)={history['center_patch'][-1]:.3f}; \"\n",
    "              f\"norm(mask_token)={history['mask_token'][-1]:.3f}\")\n",
    "    return history\n",
    "\n",
    "\n",
    "class MaskTokens(keras.layers.Layer):\n",
    "    def __init__(self, d_model, init_std=0.02, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.init_std = init_std\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mask_token = self.add_weight(\n",
    "            name=\"mask_token\",\n",
    "            shape=(self.d_model,),\n",
    "            initializer=keras.initializers.RandomNormal(stddev=self.init_std),\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, embeddings, mask_bool):\n",
    "        float_mask = tf.cast(mask_bool, embeddings.dtype)[..., None]\n",
    "        return embeddings * (1.0 - float_mask) + self.mask_token[None, None, :] * float_mask\n",
    "        \n",
    "masker = MaskTokens(d_model, name=\"particle_masker\")\n",
    "\n",
    "center_cls = tf.Variable(tf.zeros([d_proj]), trainable=False)\n",
    "center_patch = tf.Variable(tf.zeros([d_proj]), trainable=False)\n",
    "\n",
    "history_ibot_5class = train_ibot(x_train=x_train,\n",
    "                                 epochs=30,\n",
    "                                 batch_size=256,\n",
    "                                 optimizer=keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=4e-3),\n",
    "                                 #optimizer=keras.optimizers.Adam(1e-3),\n",
    "                                 ema_tau=0.996,\n",
    "                                 student=student,\n",
    "                                 teacher=teacher,\n",
    "                                 proj_head_s=proj_head_s,\n",
    "                                 proj_head_t=proj_head_t,\n",
    "                                 mask_ratio_ibot=0.4,\n",
    "                                 masker=masker,\n",
    "                                 center_cls=center_cls,\n",
    "                                 center_patch=center_patch,\n",
    "                                 center_beta=0.9,\n",
    "                                 temp_t=0.04,\n",
    "                                 temp_s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ibot_training(history):\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(8, 4), sharex=False)\n",
    "    ax[0,0].plot(history[\"loss_total\"])\n",
    "    ax[0,0].set_title(\"loss (total)\")\n",
    "    ax[0,0].set_xlabel(\"epoch\")\n",
    "\n",
    "    ax[0,1].plot(history[\"loss_cls\"])\n",
    "    ax[0,1].set_title(\"loss (cross-view CLS)\")\n",
    "    ax[0,1].set_xlabel(\"epoch\")\n",
    "\n",
    "    ax[0,2].plot(history[\"loss_patch\"])\n",
    "    ax[0,2].set_title(\"loss (masked patch)\")\n",
    "    ax[0,2].set_xlabel(\"epoch\")\n",
    "\n",
    "    ax[1,0].plot(history[\"entropy_teacher\"], label=\"teacher\")\n",
    "    ax[1,0].plot(history[\"entropy_student\"], label=\"student\")\n",
    "    ax[1,0].set_title(\"teacher/student entropy\")\n",
    "    ax[1,0].set_xlabel(\"epoch\")\n",
    "    ax[1,0].legend(fontsize=7)\n",
    "\n",
    "    ax[1,1].plot(history[\"center_cls\"], label=\"CLS\")\n",
    "    ax[1,1].plot(history[\"center_patch\"], label=\"patch\")\n",
    "    ax[1,1].set_title(\"teacher centering norms\")\n",
    "    ax[1,1].set_xlabel(\"epoch\")\n",
    "    ax[1,1].legend(fontsize=7)\n",
    "\n",
    "    ax[1,2].plot(history[\"mask_token\"])\n",
    "    ax[1,2].set_title(\"learnable pT mask token\")\n",
    "    ax[1,2].set_xlabel(\"epoch\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_ibot_training(history_ibot_5class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(transformer, n_heads, n_layers):\n",
    "    last_block = transformer.get_layer(f\"block_{n_layers-1}\")\n",
    "    mha_last = last_block.mha\n",
    "    #heads = getattr(mha_last, \"num_heads\", getattr(mha_last, \"_num_heads\"))\n",
    "\n",
    "    # rebuild model with attention scores as output\n",
    "    x_in = transformer.input\n",
    "    x = transformer.get_layer(\"token_embedding\")(x_in)\n",
    "    x = transformer.get_layer(\"prepend_cls\")(x)\n",
    "    for i in range(n_layers-1):\n",
    "        x = transformer.get_layer(f\"block_{i}\")(x)\n",
    "    _, attn_scores = mha_last(x, x, return_attention_scores=True)\n",
    "    model_re = tf.keras.Model(x_in, attn_scores)\n",
    "\n",
    "    class_names = ['q', 'g', 'W', 'Z', 't']\n",
    "    rows = 1 + n_heads\n",
    "    fig, axes = plt.subplots(rows, 5, figsize=(16, 3*rows))\n",
    "\n",
    "    for col, cname in enumerate(class_names):\n",
    "        idx = np.where(y_train.argmax(axis=1)==col)[0][0]\n",
    "        jet = x_train[idx][None, ...].astype(\"float32\") # (1,30,4)\n",
    "        scores = model_re(jet).numpy()[0] # (n_heads, 31, 31)\n",
    "        QUERYcls_KEYpatch = scores[:, 0, 1:] # (n_heads, 30)\n",
    "\n",
    "        eta, phi, pt, valid = jet[0].T\n",
    "        eta, phi, pt = eta[valid==1], phi[valid==1], pt[valid==1]\n",
    "        QUERYcls_KEYpatch = QUERYcls_KEYpatch[:, valid==1]\n",
    "\n",
    "        ax0 = axes[0, col]\n",
    "        ax0.scatter(eta, phi, color=\"C0\", s=pt*1e4, alpha=0.5)\n",
    "        ax0.set_title(f\"{cname} [raw input, fixed alpha=0.5]\")\n",
    "        ax0.set_xlim(-0.4, 0.4)\n",
    "        ax0.set_ylim(-0.4, 0.4)\n",
    "\n",
    "        for h in range(n_heads):\n",
    "            w = QUERYcls_KEYpatch[h]\n",
    "            w = (w - w.min())/(w.ptp() + 1e-8) # normalized\n",
    "            ax = axes[1+h, col]\n",
    "            ax.scatter(eta, phi, color=f\"C{h+1}\", s=pt*1e4, alpha=w)\n",
    "            if col==0: ax.set_ylabel(f\"head {h}\")\n",
    "            ax.set_title(f\"head{h}, alpha=self-attention\")\n",
    "            ax.set_xlim(-0.4, 0.4)\n",
    "            ax.set_ylim(-0.4, 0.4)\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(5):\n",
    "            axes[r,c].set_xlabel(\"eta_rel\")\n",
    "            axes[r,c].set_ylabel(\"phi_rel\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_attention(transformer=student, n_heads=n_heads, n_layers=n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9490121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_softmax_prob_cls(n_samples, student, teacher, proj_head_s, proj_head_t, center_cls, temp_t, temp_s):\n",
    "    x_sample = x_test[:n_samples].astype(\"float32\")\n",
    "\n",
    "    cls_t, _ = teacher(x_sample, training=False)\n",
    "    z_teacher = proj_head_t(tf.expand_dims(cls_t, 1))[:, 0, :]\n",
    "    prob_t = tf.nn.softmax((z_teacher - center_cls) / temp_t, axis=-1)\n",
    "    mean_prob_t = tf.reduce_mean(prob_t, axis=0).numpy()\n",
    "\n",
    "    cls_s, _ = student(x_sample, training=False)\n",
    "    z_student = proj_head_s(tf.expand_dims(cls_s, 1))[:, 0, :]\n",
    "    prob_s = tf.nn.softmax(z_student / temp_s, axis=-1)\n",
    "    mean_prob_s = tf.reduce_mean(prob_s, axis=0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(np.arange(z_teacher.shape[-1]), mean_prob_t, label=\"teacher\", alpha=0.5)\n",
    "    plt.bar(np.arange(z_student.shape[-1]), mean_prob_s, label=\"student\", alpha=0.5)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(1e-6, 10)\n",
    "    plt.legend()\n",
    "    plt.title(\"mean softmax prob for CLS token per projection dim\")\n",
    "    plt.xlabel(\"K-dim\")\n",
    "\n",
    "plot_softmax_prob_cls(n_samples=5000,\n",
    "                      student=student, teacher=teacher,\n",
    "                      proj_head_s=proj_head_s, proj_head_t=proj_head_t,\n",
    "                      center_cls=center_cls, temp_t=0.04, temp_s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tSNE_cls(n_samples, student, proj_head_s):\n",
    "    x_sample = x_train[:n_samples].astype(\"float32\")\n",
    "    y_sample = y_train[:n_samples].argmax(1)\n",
    "    palette = plt.cm.Set1(np.linspace(0, 1, 5))\n",
    "    class_names = ['q', 'g', 'W', 'Z', 't']\n",
    "\n",
    "    cls_s, _ = student(x_sample, training=False)\n",
    "    z_backbone = cls_s.numpy()  # (N, d_model)\n",
    "\n",
    "    tsne_backbone = TSNE(n_components=2, random_state=42).fit_transform(z_backbone)\n",
    "\n",
    "    tsne_projection = None\n",
    "    if proj_head_s is not None:\n",
    "        z_projection = proj_head_s(tf.expand_dims(cls_s, 1))[:, 0, :]  # (N, d_proj)\n",
    "        z_projection = z_projection.numpy()\n",
    "        tsne_projection = TSNE(n_components=2, random_state=42).fit_transform(z_projection)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2 if tsne_projection is not None else 1, figsize=(12, 6))\n",
    "\n",
    "    ax0 = ax[0] if isinstance(ax, np.ndarray) else ax\n",
    "    for k, c in enumerate(class_names):\n",
    "        ax0.scatter(tsne_backbone[y_sample == k, 0], tsne_backbone[y_sample == k, 1],\n",
    "                    s=16, label=c, color=palette[k], alpha=0.5)\n",
    "    ax0.set_title(\"t-SNE scatter (CLS token from backbone)\")\n",
    "    ax0.legend(markerscale=2.5)\n",
    "\n",
    "    if tsne_projection is not None and isinstance(ax, np.ndarray):\n",
    "        for k, c in enumerate(class_names):\n",
    "            ax[1].scatter(tsne_projection[y_sample == k, 0], tsne_projection[y_sample == k, 1],\n",
    "                          s=16, label=c, color=palette[k], alpha=0.5)\n",
    "        ax[1].set_title(\"t-SNE scatter (CLS token from projection)\")\n",
    "        ax[1].legend(markerscale=2.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_tSNE_cls(n_samples=5000, student=student, proj_head_s=proj_head_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cls_train = student.predict(x_train)[0]\n",
    "z_cls_test = student.predict(x_test)[0]\n",
    "\n",
    "y_train_idx = y_train.argmax(1)\n",
    "y_test_idx = y_test.argmax(1)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=20).fit(z_cls_train, y_train_idx)\n",
    "acc_knn = accuracy_score(y_test_idx, knn.predict(z_cls_test))\n",
    "\n",
    "scaler = StandardScaler().fit(z_cls_train)\n",
    "Z_cls_train, Z_cls_test = scaler.transform(z_cls_train), scaler.transform(z_cls_test)\n",
    "logreg = LogisticRegression(max_iter=1000).fit(Z_cls_train, y_train_idx)\n",
    "acc_linear = accuracy_score(y_test_idx, logreg.predict(Z_cls_test))\n",
    "\n",
    "print(f\"k-NN acc: {acc_knn:.3f}\")\n",
    "print(f\"linear acc: {acc_linear:.3f}\")\n",
    "\n",
    "knn_p = knn.predict_proba(z_cls_test)\n",
    "linear_p = logreg.predict_proba(Z_cls_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "colors = plt.cm.Set1(np.linspace(0,1,5))\n",
    "class_names = ['q','g','W','Z','t']\n",
    "\n",
    "for k,c in enumerate(class_names):\n",
    "    fpr_k, tpr_k, _ = roc_curve(y_test[:,k], knn_p[:,k])\n",
    "    auc_k = auc(fpr_k, tpr_k)\n",
    "    plt.plot(tpr_k, fpr_k, color=colors[k], label=f\"{c} [k-NN] ({auc_k:.3f})\", linestyle=\"-\", lw=1.5)\n",
    "\n",
    "    fpr_l, tpr_l, _ = roc_curve(y_test[:,k], linear_p[:,k])\n",
    "    auc_l = auc(fpr_l, tpr_l)\n",
    "    plt.plot(tpr_l, fpr_l, color=colors[k], label=f\"{c} [linear] ({auc_l:.3f})\", linestyle=\"--\", lw=1.5)\n",
    "\n",
    "plt.xlabel(\"TPR\", size=16)\n",
    "plt.ylabel(\"FPR\", size=16)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-4,1)\n",
    "plt.title(\"k-NN/linear on CLS in frozen backbone embedding\", size=16)\n",
    "plt.legend(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- classifier on backbone embedding (no fine tuning) vs standalone classifier\n",
    "def build_mlp(dim_in, n_classes=5, name=\"mlp\"):\n",
    "    x_in = Input(shape=(dim_in,))\n",
    "    x = Dense(dim_in*3, activation='relu')(x_in)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(dim_in*3, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(n_classes, activation='softmax')(x)\n",
    "    return tf.keras.models.Model(x_in, x, name=name)\n",
    "\n",
    "student.trainable = False\n",
    "x_in = tf.keras.Input((30,4))\n",
    "cls, _ = student(x_in)\n",
    "mlp = build_mlp(d_model, name=\"mlp_frozen\")\n",
    "x_out = mlp(cls)\n",
    "model = tf.keras.models.Model(x_in, x_out, name=\"frozen_backbone_mlp\")\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "backbone_standalone = build_backbone(d_model, n_heads, n_layers, name=\"backbone_standalone\")\n",
    "mlp_standalone = build_mlp(d_model, name=\"mlp_standalone\")\n",
    "x_in = tf.keras.Input((30,4))\n",
    "cls, _ = backbone_standalone(x_in)\n",
    "x_out = mlp_standalone(cls)\n",
    "model_standalone = tf.keras.models.Model(x_in, x_out, name=\"standalone_backbone_mlp\")\n",
    "model_standalone.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 30\n",
    "batch = 256\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch)\n",
    "model_standalone.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_ft = build_backbone(d_model, n_heads, n_layers, name=\"backbone_finetune\")\n",
    "backbone_ft.set_weights(student.get_weights())\n",
    "mlp_ft = build_mlp(d_model, name=\"mlp_finetune\")\n",
    "\n",
    "x_in = tf.keras.Input((30,4))\n",
    "cls, _ = backbone_ft(x_in)\n",
    "x_out = mlp_ft(cls)\n",
    "model_ft = tf.keras.models.Model(x_in, x_out, name=\"finetune_backbone_mlp\")\n",
    "\n",
    "model_ft.compile(optimizer=keras.optimizers.Adam(2e-4),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_ft.fit(x_train, y_train,\n",
    "             validation_data=(x_val, y_val),\n",
    "             epochs=2, batch_size=batch)\n",
    "\n",
    "backbone_ft.trainable = False\n",
    "model_ft.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_ft.fit(x_train, y_train,\n",
    "             validation_data=(x_val, y_val),\n",
    "             initial_epoch=2, epochs=epochs, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred_ft = model_ft.predict(x_test)\n",
    "y_pred_standalone = model_standalone.predict(x_test)\n",
    "\n",
    "acc_model = accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "acc_ft = accuracy_score(y_test.argmax(axis=1), y_pred_ft.argmax(axis=1))\n",
    "acc_standalone = accuracy_score(y_test.argmax(axis=1), y_pred_standalone.argmax(axis=1))\n",
    "\n",
    "print(f\"acc: {acc_model:.3f}\")\n",
    "print(f\"finetune acc: {acc_ft:.3f}\")\n",
    "print(f\"standalone acc: {acc_standalone:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "colors = plt.cm.Set1(np.linspace(0,1,5))\n",
    "class_names = ['q','g','W','Z','t']\n",
    "\n",
    "for k,c in enumerate(class_names):\n",
    "    fpr_f, tpr_f, _ = roc_curve(y_test[:,k], y_pred[:,k])\n",
    "    auc_f = auc(fpr_f, tpr_f)\n",
    "    plt.plot(tpr_f, fpr_f, color=colors[k], lw=1.5, linestyle='-', label=f\"{c} [frozen ibot] ({auc_f:.3f})\")\n",
    "\n",
    "    fpr_ft, tpr_ft, _ = roc_curve(y_test[:,k], y_pred_ft[:,k])\n",
    "    auc_ft = auc(fpr_ft, tpr_ft)\n",
    "    plt.plot(tpr_ft, fpr_ft, color=colors[k], lw=1.5, linestyle='--', label=f\"{c} [finetune ibot] ({auc_ft:.3f})\")\n",
    "    \n",
    "    fpr_s, tpr_s, _ = roc_curve(y_test[:,k], y_pred_standalone[:,k])\n",
    "    auc_s = auc(fpr_s, tpr_s)\n",
    "    plt.plot(tpr_s, fpr_s, color=colors[k], lw=1.5, linestyle='dotted', label=f\"{c} [standalone] ({auc_s:.3f})\")\n",
    "\n",
    "plt.xlabel(\"TPR\", size=16)\n",
    "plt.ylabel(\"FPR\", size=16)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-4,1)\n",
    "plt.title(\"MLP on ibot-student CLS embedding\", size=16)\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be39e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tSNE_cls(n_samples=5000, student=backbone_ft, proj_head_s=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0adae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc96eded",
   "metadata": {},
   "source": [
    "## AD (train ibot with q/g only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_model=128\n",
    "#n_heads=3\n",
    "#n_layers=3\n",
    "#d_proj=64\n",
    "\n",
    "student_ad = build_backbone(d_model=d_model, n_heads=n_heads, n_layers=n_layers)\n",
    "teacher_ad = build_backbone(d_model=d_model, n_heads=n_heads, n_layers=n_layers)\n",
    "\n",
    "proj_head_s_ad = build_proj_head(d_in=d_model, d_proj=d_proj, name=\"proj_head_student\")\n",
    "proj_head_t_ad = build_proj_head(d_in=d_model, d_proj=d_proj, name=\"proj_head_teacher\")\n",
    "\n",
    "teacher_ad.set_weights(student_ad.get_weights())\n",
    "proj_head_t_ad.set_weights(proj_head_s_ad.get_weights())\n",
    "teacher_ad.trainable = False\n",
    "proj_head_t_ad.trainable = False\n",
    "\n",
    "print(student_ad.summary())\n",
    "print(proj_head_s_ad.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5551aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_mask_train = (y_train[:,0] + y_train[:,1])==1\n",
    "anomaly_mask_train = ~normal_mask_train\n",
    "\n",
    "x_train_normal = x_train[normal_mask_train]\n",
    "x_train_anomaly = x_train[anomaly_mask_train]\n",
    "\n",
    "masker_ad = MaskTokens(d_model, name=\"particle_masker\")\n",
    "center_cls_ad = tf.Variable(tf.zeros([d_proj]), trainable=False)\n",
    "center_patch_ad = tf.Variable(tf.zeros([d_proj]), trainable=False)\n",
    "\n",
    "history_ibot_2class = train_ibot(x_train=x_train_normal,\n",
    "                                 epochs=30,\n",
    "                                 batch_size=256,\n",
    "                                 optimizer=keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=4e-3),\n",
    "                                 #optimizer=keras.optimizers.Adam(2e-4),\n",
    "                                 ema_tau=0.996,\n",
    "                                 student=student_ad,\n",
    "                                 teacher=teacher_ad,\n",
    "                                 proj_head_s=proj_head_s_ad,\n",
    "                                 proj_head_t=proj_head_t_ad,\n",
    "                                 mask_ratio_ibot=0.4,\n",
    "                                 masker=masker_ad,\n",
    "                                 center_cls=center_cls_ad,\n",
    "                                 center_patch=center_patch_ad,\n",
    "                                 center_beta=0.9,\n",
    "                                 temp_t=0.04,\n",
    "                                 temp_s=0.1)\n",
    "\n",
    "plot_ibot_training(history_ibot_2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94531666",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(transformer=student_ad, n_heads=n_heads, n_layers=n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15231c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_softmax_prob_cls(n_samples=5000,\n",
    "                      student=student_ad, teacher=teacher_ad,\n",
    "                      proj_head_s=proj_head_s_ad, proj_head_t=proj_head_t_ad,\n",
    "                      center_cls=center_cls_ad, temp_t=0.04, temp_s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tSNE_cls(n_samples=5000, student=student_ad, proj_head_s=proj_head_s_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train_normal = student_ad.predict(x_train_normal)[0]\n",
    "knn_ad_bank = NearestNeighbors(n_neighbors=20, metric='euclidean').fit(z_train_normal)\n",
    "cosim_ad_bank = z_train_normal / np.linalg.norm(z_train_normal, axis=1, keepdims=True)\n",
    "M = cosim_ad_bank.shape[0]\n",
    "\n",
    "def ad_score_knn(x, student_ad, knn_ad_bank):\n",
    "    z = student_ad.predict(x)[0] \n",
    "    dist, _ = knn_ad_bank.kneighbors(z)\n",
    "    return dist.mean(axis=1)\n",
    "\n",
    "def ad_score_cosim(x, student_ad, cosim_ad_bank, sum_or_max, tau=0.04):\n",
    "    z = student_ad.predict(x)[0]\n",
    "    z = z / np.linalg.norm(z, axis=1, keepdims=True)\n",
    "    sim = np.dot(z, cosim_ad_bank.T)\n",
    "    if sum_or_max==\"sum\":\n",
    "        return -np.mean(np.exp(sim / tau), axis=1) * 1e-10\n",
    "    elif sum_or_max==\"max\":\n",
    "        return 1-np.max(sim, axis=1) \n",
    "\n",
    "normal_mask_test = (y_test[:,0]==1) | (y_test[:,1]==1)\n",
    "x_test_normal = x_test[normal_mask_test]\n",
    "score_knn_normal = ad_score_knn(x_test_normal, student_ad, knn_ad_bank)\n",
    "score_cosim_sum_normal = ad_score_cosim(x_test_normal, student_ad, cosim_ad_bank, \"sum\")\n",
    "score_cosim_max_normal = ad_score_cosim(x_test_normal, student_ad, cosim_ad_bank, \"max\")\n",
    "\n",
    "anomaly_classes = {2:'W', 3:'Z', 4:'t'}\n",
    "scores_knn_anomaly = {}\n",
    "scores_cosim_sum_anomaly = {}\n",
    "scores_cosim_max_anomaly = {}\n",
    "for i,(col,name) in enumerate(anomaly_classes.items()):\n",
    "    anomaly_mask_test = y_test[:,col]==1\n",
    "    x_test_anomaly = x_test[anomaly_mask_test]\n",
    "    scores_knn_anomaly[name] = ad_score_knn(x_test_anomaly, student_ad, knn_ad_bank)\n",
    "    scores_cosim_sum_anomaly[name] = ad_score_cosim(x_test_anomaly, student_ad, cosim_ad_bank, \"sum\")\n",
    "    scores_cosim_max_anomaly[name] = ad_score_cosim(x_test_anomaly, student_ad, cosim_ad_bank, \"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ba66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(15,3))\n",
    "\n",
    "ax[0].hist(score_knn_normal, bins=100, density=True, color='grey', alpha=0.5, label='q/g')\n",
    "for (name, score), i in zip(scores_knn_anomaly.items(), range(len(scores_knn_anomaly))):\n",
    "    ax[0].hist(score, bins=100, density=True, histtype='step', linewidth=2, color=f\"C{i}\", label=f\"{name}\")\n",
    "ax[0].set_xlabel(\"anomaly score\", size=12)\n",
    "ax[0].set_title(\"mean distance to k-NN\", size=12)\n",
    "ax[0].legend(fontsize=10)\n",
    "\n",
    "ax[1].hist(score_cosim_sum_normal, bins=100, density=True, color='grey', alpha=0.5, label='q/g')\n",
    "for (name, score), i in zip(scores_cosim_sum_anomaly.items(), range(len(scores_cosim_sum_anomaly))):\n",
    "    ax[1].hist(score, bins=100, density=True, histtype='step', linewidth=2, color=f\"C{i}\", label=f\"{name}\")\n",
    "ax[1].set_xlabel(\"anomaly score\", size=12)\n",
    "ax[1].set_title(\"cosine similarity (sum)\", size=12)\n",
    "ax[1].legend(fontsize=10)\n",
    "\n",
    "ax[2].hist(score_cosim_max_normal, bins=100, density=True, color='grey', alpha=0.5, label='q/g')\n",
    "for (name, score), i in zip(scores_cosim_max_anomaly.items(), range(len(scores_cosim_max_anomaly))):\n",
    "    ax[2].hist(score, bins=100, density=True, histtype='step', linewidth=2, color=f\"C{i}\", label=f\"{name}\")\n",
    "ax[2].set_xlabel(\"anomaly score\", size=12)\n",
    "ax[2].set_title(\"cosine similarity (max)\", size=12)\n",
    "ax[2].set_xlim(0,0.008)\n",
    "ax[2].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554003d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "colors = [\"blue\", \"orange\", \"green\"]\n",
    "\n",
    "for i,(col,name) in enumerate(anomaly_classes.items()):\n",
    "    y_true = np.concatenate([np.zeros_like(score_knn_normal), np.ones_like(scores_knn_anomaly[name])])\n",
    "    scores = np.concatenate([score_knn_normal, scores_knn_anomaly[name]])\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    auc_ = auc(fpr, tpr)\n",
    "    plt.plot(tpr, fpr, color=colors[i], label=f\"{name} [k-NN] ({auc_:.3f})\", linestyle=\"-\", lw=2)\n",
    "\n",
    "    y_true = np.concatenate([np.zeros_like(score_cosim_sum_normal), np.ones_like(scores_cosim_sum_anomaly[name])])\n",
    "    scores = np.concatenate([score_cosim_sum_normal, scores_cosim_sum_anomaly[name]])\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    auc_ = auc(fpr, tpr)\n",
    "    plt.plot(tpr, fpr, color=colors[i], label=f\"{name} [cosim (sum)] ({auc_:.3f})\", linestyle=\"--\", lw=2)\n",
    "\n",
    "    y_true = np.concatenate([np.zeros_like(score_cosim_max_normal), np.ones_like(scores_cosim_max_anomaly[name])])\n",
    "    scores = np.concatenate([score_cosim_max_normal, scores_cosim_max_anomaly[name]])\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    auc_ = auc(fpr, tpr)\n",
    "    plt.plot(tpr, fpr, color=colors[i], label=f\"{name} [cosim (max)] ({auc_:.3f})\", linestyle=\"dotted\", lw=2)\n",
    "\n",
    "plt.xlabel(\"TPR\", size=16)\n",
    "plt.ylabel(\"FPR\", size=16)\n",
    "#plt.yscale(\"log\")\n",
    "#plt.ylim(1e-4,1)\n",
    "plt.title(\"AD on backbone CLS embedding\", size=16)\n",
    "plt.legend(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff7ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ac03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
